{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import LOM\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "Generate three types of process through kernels:\n",
    "- Gaussian -1\n",
    "- Increasing/Decreasing linear trend\n",
    "- Gaussian -2\n",
    "- Generate latent process and a random phi add noise\n",
    "- Label processes as coming from increasing trend-G1 as 1’s and decreasing trend-G1/G2 as 0’s\n",
    "- Increasing n, check classification accuracy \n",
    "- Compare with Logistic regression and LDA: Stretch goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Linear increasing kernel\n",
    "import GPy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "kenrel = GPy.kern.Linear(1,variances=3)  * GPy.kern.Brownian(1,variance=2) \n",
    "kenrel = GPy.kern.Linear(1,variances=3) * GPy.kern.Brownian(1,variance=3) + GPy.kern.Matern52(1,variance=2,lengthscale=3) \n",
    "Matern1 = GPy.kern.PeriodicMatern32(1,lengthscale=3,variance=2)\n",
    "Cosine1 = GPy.kern.Cosine(1,lengthscale=3,variance=2)\n",
    "gaussian = GPy.kern.ExpQuad(1,lengthscale=3,variance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale(d):\n",
    "    return (d - d.min())/(d.max() - d.min())\n",
    "\n",
    "def generatelatent(N,noiselevel=0.5):\n",
    "    x = np.linspace(0,N-1,N)\n",
    "    u1 = np.random.multivariate_normal(mean=np.zeros(N).flatten(),cov=kenrel.K(x.reshape(N,1))) + noiselevel* np.random.normal(0,1,N)\n",
    "    u2 = np.random.multivariate_normal(mean=np.zeros(N).flatten(),cov=gaussian.K(x.reshape(N,1))) + noiselevel* np.random.normal(0,1,N)\n",
    "    g1 = np.random.multivariate_normal(mean=np.ones(N).flatten(),cov=Matern1.K(x.reshape(N,1))) + noiselevel* np.random.normal(0,1,N)\n",
    "    g2 = np.random.multivariate_normal(mean=np.ones(N).flatten(),cov=Cosine1.K(x.reshape(N,1))) + noiselevel*np.random.normal(0,1,N)\n",
    "    return scale(u1),scale(u2),scale(g1),scale(g2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know our latent processes: g1,g2,increasingTrend,decreasingTrend\n",
    "We sample phi and then combine these to generate data such that:\n",
    "    - with probability 0.5:\n",
    "        get g1 and increasing trend set Label to 1\n",
    "     - With probability 0.5:\n",
    "         get g1 or g2 with equal probability combine with decreasing trend and set label to -1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateData(W,phi,latents,S=100,C=3,P=2,N=100):\n",
    "    #phi = np.random.normal(loc=0,scale=1,size=(C,P))\n",
    "    phiBar = np.matrix(np.kron(phi,np.eye(N)))\n",
    "    B = np.ones(S)\n",
    "    #W = np.random.rand(P*N)\n",
    "    uBar = np.matrix(np.ones((S,N*P)))\n",
    "    l = np.zeros(S)\n",
    "    L = np.zeros(S)\n",
    "    for s in range(S):\n",
    "        if np.random.rand() < 0.4:\n",
    "            L[s] = 1\n",
    "            us = np.hstack([a for i,a in enumerate(latents) if i!=1 ])\n",
    "        else:\n",
    "            L[s] = -1\n",
    "            us = np.hstack([a for i,a in enumerate(latents) if i!=1 ])\n",
    "        uBar[s,:] = us\n",
    "        l[s] = np.dot(uBar[s,:] ,W) + B[s]\n",
    "        #L[s] = guessLabel(l[s])\n",
    "    YBar = uBar * phiBar.T + np.random.normal(0,1)\n",
    "    return YBar,L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing Infra:\n",
    "\n",
    "## Testing the effect of inducing Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      " C: 1\n",
      "Concatenated Latent Gaussian Processes:\n",
      "(500, 400) (400, 400) 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /m/home/home8/81/nautiys1/unix/jupyter/venv/lib/python2.7/site-packages/ipykernel/__main__.py:8: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /m/home/home8/81/nautiys1/unix/jupyter/venv/lib/python2.7/site-packages/ipykernel/__main__.py:9: RuntimeWarning:covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20 40 60 80 100 120 140 160 180 Result: (array([-1.]), array([80])) (array([-1.,  1.]), array([56, 24]))\n",
      "Concatenated Latent Gaussian Processes:\n",
      "(500, 400) (400, 400) 0 20 40 60 80 100 120 140 160 180 Result: (array([-1.]), array([80])) (array([-1.,  1.]), array([41, 39]))\n",
      "Concatenated Latent Gaussian Processes:\n",
      "(500, 400) (400, 400) 0 20 40 60 80 100 120 140 160 180 Result: (array([-1.]), array([80])) (array([-1.,  1.]), array([53, 27]))\n",
      "Concatenated Latent Gaussian Processes:\n",
      "(500, 400) (400, 400) 0 20 40 60 80 100 120 140 160 180 Result: (array([-1.]), array([80])) (array([-1.,  1.]), array([41, 39]))\n",
      "Concatenated Latent Gaussian Processes:\n",
      "(500, 400) (400, 400) 0 20 40 60 80 100 120 140 160 180 Result: (array([-1.]), array([80])) (array([-1.,  1.]), array([40, 40]))\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0] \n",
      "[0.69999999999999996, 0.51249999999999996, 0.66249999999999998, 0.51249999999999996, 0.5]\n",
      "\n",
      "---\n",
      " C: 2\n",
      "Concatenated Latent Gaussian Processes:\n",
      "(500, 400) (400, 400) 0 20 40 60 80 100 120 140 160 180 Result: (array([-1.]), array([80])) (array([-1.,  1.]), array([54, 26]))\n",
      "Concatenated Latent Gaussian Processes:\n",
      "(500, 400) (400, 400) 0 20 40 60 80 100 120 140 160 180 Result: (array([-1.]), array([80])) (array([-1.,  1.]), array([47, 33]))\n",
      "Concatenated Latent Gaussian Processes:\n",
      "(500, 400) (400, 400) 0 20 40 60 80 100 120 140 160 180"
     ]
    }
   ],
   "source": [
    "Cs = range(1,10)\n",
    "N = 100\n",
    "actualP = 3\n",
    "guessedP = 5\n",
    "S= 200\n",
    "\n",
    "ind = 0.8\n",
    "accuracies = {}\n",
    "f1Scores = {}\n",
    "for c in Cs:\n",
    "    f1=[]\n",
    "    accu=[]\n",
    "    print \"\\n---\\n C:\",c\n",
    "    W = np.random.rand(actualP*N)\n",
    "    phi = np.random.normal(loc=0,scale=1,size=(c,actualP))\n",
    "    for times in range(5):\n",
    "        #generate Latent Proceses:\n",
    "        latent = generatelatent(N=N)\n",
    "        #generate data\n",
    "        YBar,L = generateData(W,phi,latents=latent,C=c,N=N,P=actualP,S=S)\n",
    "        # create train test split\n",
    "        Y_train, Y_test, L_train, L_test = train_test_split(YBar, L, test_size=0.4, random_state=0)\n",
    "        # fit the model\n",
    "        myModel = LOM.LOM(Y=Y_train,L=L_train,N=N,C=c,S=L_train.shape[0])\n",
    "        myModel.fit(n=int(N*ind),iters=200,P=guessedP)\n",
    "        # predict for test\n",
    "        predictions,Expresults, _ = myModel.predict(Y_test)\n",
    "        # calcualte accuracy\n",
    "        print \"Result:\",np.unique(predictions,return_counts=True),np.unique(L_test,return_counts=True)\n",
    "        f1.append(metrics.f1_score(y_pred=predictions,y_true=L_test))\n",
    "        accu.append(metrics.accuracy_score(y_pred=predictions,y_true=L_test))\n",
    "    print f1,\"\\n\",accu\n",
    "    f1Scores[c] = np.mean(f1)\n",
    "    accuracies[c] = np.mean(accu)\n",
    "#plt.plot(induction,results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "YBar = pickle.load(open(\"YBar.data\",\"rb\"))\n",
    "L = pickle.load(open(\"Labels.data\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YBar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ea5f0eaa2e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYBar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmyModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLOM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "Y_train, Y_test, L_train, L_test = train_test_split(YBar, L, test_size=0.4, random_state=0)\n",
    "myModel = LOM.LOM(Y=Y_train,L=L_train,N=60,C=5,S=L_train.shape[0])\n",
    "myModel.fit(n=int(N*ind),iters=200,P=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
